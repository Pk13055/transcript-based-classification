{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Activation, Dense, Input, LSTM\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.models import load_model\n",
    "from keras.losses import binary_crossentropy,mean_squared_error\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.utils.np_utils as np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.core.debugger import set_trace\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def split_dataset(filename, test_split_ratio):\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[1], df[0], test_size=test_split_ratio, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " usermention as a woman you shouldnt complain about cleaning up your house amp as a man you should always take the trash out\n",
      "20796\n",
      "22141\n",
      "20796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 4, 300)            6642300   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4, 300)            0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 6,742,079\n",
      "Trainable params: 6,742,079\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(22304, 4)\n",
      "(22304, 3)\n",
      "Epoch 1/2\n",
      "22304/22304 [==============================] - 79s 4ms/step - loss: 0.5424 - acc: 0.7919\n",
      "Epoch 2/2\n",
      "22304/22304 [==============================] - 76s 3ms/step - loss: 0.4415 - acc: 0.8321\n",
      "(22304, 3)\n",
      "(2479, 3)\n",
      "2052\n",
      "2479\n",
      "0.827753126260589\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv(\"clean_data.csv\", header=None)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[1], df[0], test_size=0.10, random_state=42)\n",
    "\n",
    "\n",
    "    print((X_train[0]))\n",
    "\n",
    "    embedding = np.load('300_glove_davidson.npy')\n",
    "    \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(X_train)\n",
    "    vocab_size = len(t.word_index) + 1\n",
    "    print(vocab_size)\n",
    "    encoded_docs = t.texts_to_sequences(X_train)\n",
    "\n",
    "    max_length = 4\n",
    "    padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "    '''\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(X_test)\n",
    "    vocab_size_test = len(t.word_index) + 1'''\n",
    "    encoded_docs_test = t.texts_to_sequences(X_test)\n",
    "\n",
    "    padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')\n",
    "    \n",
    "    model=Sequential()  \n",
    "\n",
    "    # EMBEDDING LAYER - DISTRIBUTED REPRESENTATION OF TWEETS \n",
    "    # EMBEDDINGS - GLOVE 100 dimensions further trained on davidson and heot dataset after proper preprocessing\n",
    "    # EMBEDDING DIMENSION = 100\n",
    "    print(np.shape(embedding)[0])\n",
    "    print(vocab_size)\n",
    "    model.add(Embedding(22141, 300, weights=[embedding], input_length=4, name='embedding_layer'))\n",
    "\n",
    "    # Dropout Layer to reduce overfitting \n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    # LSTM Layer (2 LSTM layers preferable) - Units : 64\n",
    "    model.add(LSTM(64,dropout_W=0.2,dropout_U=0.2))\n",
    "\n",
    "    #Series of dense layers  \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax',name='last'))\n",
    "\n",
    "    # Compiling Model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # ONE HOT ENCODING - for traing and testing data and transfer learning\n",
    "    y_train1 = np_utils.to_categorical(y_train, num_classes=3)\n",
    "    y_test1 = np_utils.to_categorical(y_test, num_classes=3)\n",
    "    #Y_transfer_learning_train=np_utils.to_categorical(Y_transfer_learning_train,num_classes=3)\n",
    "    #Y_transfer_learning_test=np_utils.to_categorical(Y_transfer_learning_test,num_classes=3)\n",
    "    print(np.shape(padded_docs))\n",
    "    # Training Model\n",
    "    #model.fit(padded_docs,y_train,epochs=1,batch_size=128)\n",
    "    print(np.shape(y_train1))\n",
    "    history = model.fit(padded_docs, y_train1, batch_size=32,epochs=2,validation_split=0.0)\n",
    "    print(np.shape(y_train1))\n",
    "    #testing\n",
    "    co=0\n",
    "    tot=0\n",
    "    ans = model.predict([padded_docs_test])\n",
    "    print(np.shape(y_test1))\n",
    "    for i in range(len(y_test1)):\n",
    "        tot+=1\n",
    "        #print(\"X=%s, Predicted=%s, 1=%s, 2=%s\" % (y_test1[i], ans[i], np.argmax(y_test1[i]), np.argmax(ans[i])))\n",
    "        if np.argmax(ans[i]) == np.argmax(y_test1[i]):\n",
    "            co+=1\n",
    "    print(co)\n",
    "    print(tot)\n",
    "    acc = co / tot\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

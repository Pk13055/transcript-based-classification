{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'youtube' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-76fa695ba7c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0myoutube_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdavidson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'modified_davidson.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myoutube\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdavidson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# in case of davidson and youtube, [a, b, c] is abusive, hate, clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'youtube' is not defined"
     ]
    }
   ],
   "source": [
    "# loading the datasets\n",
    "youtube_, davidson = pd.read_csv('final.csv'), pd.read_csv('modified_davidson.csv')\n",
    "print(youtube.head(), davidson.head())\n",
    "# in case of davidson and youtube, [a, b, c] is abusive, hate, clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           text\n",
      "label          \n",
      "[0, 0, 1]  5298\n",
      "[0, 1, 0]   244\n",
      "[1, 0, 0]   456\n",
      "[1, 1, 0]   824             text\n",
      "label           \n",
      "[0, 0, 1]   4162\n",
      "[0, 1, 0]   1430\n",
      "[1, 0, 0]  19189\n"
     ]
    }
   ],
   "source": [
    "# creating the text dataset for training\n",
    "youtube = youtube_[['text', 'label']]\n",
    "print(youtube.groupby('label').count(), davidson.groupby('label').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen, there are 4 categories, viz. Abusive, Hate, abusive + hate, and clean.\n",
    "\n",
    "| Category | Samples |\n",
    "| :---:    | :---: |\n",
    "| Abusive |  19189 + 456 | \n",
    "| Hate | 1430 + 244  |\n",
    "| Abusive + Hate | 0 + 824 |\n",
    "| Clean | 4162 + 5298 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[0, 0, 1]</th>\n",
       "      <td>5222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0, 1, 0]</th>\n",
       "      <td>1479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1, 0, 0]</th>\n",
       "      <td>19280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[1, 1, 0]</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            text\n",
       "label           \n",
       "[0, 0, 1]   5222\n",
       "[0, 1, 0]   1479\n",
       "[1, 0, 0]  19280\n",
       "[1, 1, 0]    165"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED_PERCENT = 0.2\n",
    "test_sample = youtube.groupby('label').apply(lambda x: x.sample(frac=SEED_PERCENT))\n",
    "test_sample.reset_index(drop=True, inplace=True)\n",
    "dataset = pd.merge(davidson, test_sample, on=['text', 'label'], how='outer')\n",
    "dataset.to_csv('comb_data.csv', index=False)\n",
    "dataset.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now that the dataset is prepared, we can proceed to training and compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>woman shouldnt complain clean hous amp man alw...</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boy dat coldtyga dwn bad cuffin dat hoe st place</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dawgyou ever fuck bitch start cri confus shit</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>look like tranni</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shit hear might true might faker bitch told ya</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      label\n",
       "0  woman shouldnt complain clean hous amp man alw...  [0, 0, 1]\n",
       "1   boy dat coldtyga dwn bad cuffin dat hoe st place  [1, 0, 0]\n",
       "2      dawgyou ever fuck bitch start cri confus shit  [1, 0, 0]\n",
       "3                                   look like tranni  [1, 0, 0]\n",
       "4     shit hear might true might faker bitch told ya  [1, 0, 0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('comb_data_cleaned.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19210, 256)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "embedding_ = np.load('embeddings_latest.npy')\n",
    "embedding_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Embedding_Layer (Embedding)  (None, 512, 256)          4917760   \n",
      "_________________________________________________________________\n",
      "Dropout (Dropout)            (None, 512, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 1024)              3149824   \n",
      "_________________________________________________________________\n",
      "Dense_64 (Dense)             (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "Dense_32 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "Dense_3 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 8,135,363\n",
      "Trainable params: 8,135,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model, save_model\n",
    "from keras.layers import Embedding, Dropout, Bidirectional, LSTM, Dense\n",
    "INPUT_SIZE = 512\n",
    "def get_model(embedding, vocab_size=INPUT_SIZE):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(*embedding.shape, weights=[embedding], input_length=vocab_size, name='Embedding_Layer'))\n",
    "    model.add(Dropout(0.2, name=\"Dropout\"))\n",
    "    model.add(Bidirectional(LSTM(vocab_size, dropout=0.1, recurrent_dropout=0.25, name=\"Bi-LSTM_Layer_1\")))\n",
    "\n",
    "    model.add(Dense(64, activation='relu', name=\"Dense_64\"))\n",
    "    model.add(Dense(32, activation='relu', name=\"Dense_32\"))\n",
    "    model.add(Dense(3, activation='sigmoid',name='Dense_3'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy', 'accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model(embedding_, INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4979, 512), (0, 512))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "dataset = pd.read_csv('training.csv')\n",
    "_X, _y = dataset['text'].astype('str'), dataset['label']\n",
    "tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))\n",
    "X = tokenizer.texts_to_matrix(_X)\n",
    "y = np.array(list(map(lambda label: literal_eval(label), _y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=69, test_size=0.0)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15753eb8f5fb4f799e00654bf809e290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=4, style=ProgressStyle(description_width='init…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c69de9b99948b2add37c48ce3d9590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=3983, style=ProgressStyle(description_width='in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import datetime\n",
    "history = model.fit(X_train, y_train, batch_size=16, epochs=4, validation_split=0.2, verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "model_name = f\"model_{datetime.datetime.now()}.h5\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(32, 16))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch\"), plt.ylabel(\"Accuracy\")\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss Decay')\n",
    "plt.ylabel('Loss'), plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and testing\n",
    "\n",
    "Now that we've trained the model, it's time to test it against some real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "test_df = pd.read_csv('final.csv')[['text', 'label']]\n",
    "model = load_model(model_name)  # change this to the corresponding model file\n",
    "# plot_model(model, to_file='model.png'\n",
    "print(test_df.head())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X_test, y_test = test_df.text, np.array([literal_eval(_) for _ in test_df.label.values.tolist()])\n",
    "X_test = tokenizer.texts_to_matrix(_X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from keras.utils import to_categorical\n",
    "y_hat_ = to_categorical([np.argmax(_) for _ in y_hat], num_classes=3).astype('int')\n",
    "print(classification_report(y_test, y_hat_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

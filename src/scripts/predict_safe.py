#!/usr/bin/env python3
# coding: utf-8
"""
    :brief: Use this script to predict the safety of a video given
    its YouTube url.
    :usage: ./predict_safe.py -m /path/to/model -l <youtube url>
    :author: Pratik K
"""
import argparse
from io import BytesIO
import os
import pickle
import re
from string import punctuation

from keras.models import load_model
from keras.preprocessing.text import Tokenizer
from keras.utils import to_categorical
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
import pandas as pd
import pycaption
import requests
import youtube_dl


# download and prepare resources
corpa = ['wordnet', 'stopwords', 'punkt']
[nltk.download(corpus) for corpus in corpa]
en_words = set(stopwords.words('english')) | set(punctuation)
pattern = re.compile(r'[^\x00-\x7F\x80-\xFF\u0100-\u017F\u0180-\u024F\u1E00-\u1EFF]')


def collect_args() -> argparse.Namespace:
    """Parse command line arguments

    :return args: argparse.Namespace -> the arguments
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", type=str, required=False,
                        help="Model to use to run the evaluation")
    parser.add_argument("-l", "--link", type=str, required=False,
                        help="YouTube link of the video")
    parser.add_argument("-t", "--tokenizer", type=str, help="Tokenizer path",
                        default=os.path.join(os.getcwd(), 'tokenizer.pkl'))
    parser.add_argument('-r', "--results", type=str, help="Supply the "
                        "results.csv generated by a prior run", required=False)
    args = parser.parse_args()
    return args


def parse_transcript(raw: pycaption.base.CaptionList) -> pd.DataFrame:
    """Given WebVTTReader caption list, parse
    to get a DataFrame

    :param raw: WebVTTReader caption
    :return df: DataFrame with cleaned caption data
    """
    captions = [[_.start * 1e-6, _.end * 1e-6, re.sub(pattern, ' ', _.get_text())] for _ in raw]
    df = pd.DataFrame(captions, columns=['start', 'end', 'text'])
    return df


def get_transcript(link: str) -> pd.DataFrame:
    """Parse and collect transcript given link

    :param link: str -> YouTube uRL of the resource
    :return df: pd.DataFrame -> DataFrame containing required data
    """
    ydl = youtube_dl.YoutubeDL({
        'subtitlesformat': 'vtt',
        'quiet': True,
        'forcetitle': True,
        'writeautomaticsub': True,
        'simulate': True
    })
    raw = ydl.extract_info(link, download=False)
    unique_id, title = raw['display_id'], raw['title']
    print(f"Video - {unique_id}: {title}")
    try:
        sub_url = raw['requested_subtitles']['en']['url']
        resp = requests.get(sub_url, stream=True)
        bytes_ = BytesIO()
        [bytes_.write(block) for block in resp.iter_content(1024)]
        bytes_.seek(0)
        arr = pycaption.WebVTTReader().read(bytes_.read().decode('utf-8'))
        transcript = arr.get_captions('en-US')
        df = parse_transcript(transcript)
    except KeyError:
        print(f"{title} [{unique_id}] has no English subtitles! Exiting ...")
        return pd.DataFrame([], columns=['text'])
    return df


def parse_data(raw_data: pd.DataFrame, wnl: WordNetLemmatizer,
               ps: PorterStemmer, tokenizer: Tokenizer) -> np.array:
    """Parses raw data into a matrix representation

    :param raw_data: pd.DataFrame -> Raw dataframe of text
    :return encoding_: np.array -> Encoded representation
    """
    _X = raw_data.map(lambda x:
                      ' '.join(list(filter(lambda y: y not in en_words,
                                                [ps.stem(wnl.lemmatize(token))
                                                 for token in word_tokenize(x)]))))
    encoding_ = tokenizer.texts_to_matrix(_X)
    return encoding_


def generate_results(data: pd.DataFrame, verbose: bool=True) -> None:
    """Generate results given test label and meta data

    :param data: pd.DataFrame -> the labeled transcript data
    :return None:
    """
    categories = data.label.value_counts()
    if verbose: print(categories)

    mapping = { k: v for k, v in zip(['Abusive', 'Clean', 'Hate'], (255 * np.eye(3)).astype('int'))}
    colors = data.label.map(lambda x: mapping[x])
    map_len = int(data.end.max() * 1000 + 1)
    heatmap = np.zeros((map_len, 3))

    idxs = data[['start', 'end']].T.apply(lambda x: (x * 1000).astype('int')).values.tolist()
    slices = [slice(start, end) for start, end in zip(*idxs)]
    colors = data.label.map(lambda x: mapping[x])

    def pop(slice_, color):
        heatmap[slice_] += color

    [pop(slice_, color) for slice_, color in zip(slices, colors)]
    extra_ele = heatmap.shape[0] % 1000
    n_iter = int((heatmap.shape[0] - extra_ele) / 1000)
    _extra = heatmap[-extra_ele:]

    heatmap = np.vstack([np.sum(_, axis=0) for _ in heatmap[:-extra_ele].reshape(1000, n_iter, 3)]) / 1000
    heatmap = np.vstack([heatmap, np.sum(_extra, axis=0) / extra_ele])
    heatmap = heatmap.astype('int')
    width = int(1 / 2.39 * heatmap.shape[0])
    heatmap = heatmap[np.newaxis, :].repeat(width, axis=0)

    import matplotlib.pyplot as plt
    fig = plt.figure(figsize=(15,15))
#fig.add_subplot(1, 2, 1)
    categories.plot(kind='bar')
    plt.title("Frequency Distribution")
    plt.savefig('hatef.png')
    plt.show()
    fig = plt.figure(figsize=(15,15))
#   fig.add_subplot(1, 2, 2)
    plt.imshow(heatmap, cmap='gray')
    plt.title("Content Distribution")
    plt.xticks(range(0, 1000, 20), np.linspace(data.start.min(), data.end.max(), 50).astype('int'), rotation=90)
    plt.xlabel('seconds')
    plt.savefig('hate.png')
    plt.show()


def main():
    args = collect_args()

    if args.results is not None:
        data = pd.read_csv(args.results)
        generate_results(data)
        return 0

    print(f"Collecting transcript data for {args.link}")
    transcript = get_transcript(args.link)
    print(f"Transcript: {args.link}\n", transcript.head())
    raw_data = transcript['text']

    print("\n\nProcessing data ...")
    tokenizer = pickle.load(open(args.tokenizer, 'rb'))
    wnl = WordNetLemmatizer()
    ps = PorterStemmer()
    X_test = parse_data(raw_data, wnl, ps, tokenizer)
    print(f"Data dimensions: {X_test.shape}")

    print(f"\n\nLoading model {args.model.rsplit('/')[-1]}")
    model = load_model(args.model)

    print("\n\nPredicting results ...")
    _y_hat = model.predict(X_test)
    y_hat = np.argmax(_y_hat, axis=-1)
    transcript['label'] = y_hat
    transcript['label'] = transcript['label'].map(lambda x: ['Hate', 'Abusive', 'Clean'][x])
    transcript.to_csv('results.csv', index=False)

    print("Generating statistics ...")
    generate_results(transcript)


if __name__ == "__main__":
    main()
